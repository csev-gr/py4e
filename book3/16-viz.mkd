Οπτικοποίηση δεδομένων
======================

Μέχρι στιγμής μαθαίνουμε τη γλώσσα Python και πώς να χρησιμοποιούμε την Python,
το δίκτυο και τις βάσεις δεδομένων, για να χειριζόμαστε δεδομένα.

Σε αυτό το κεφάλαιο, ρίχνουμε μια ματιά σε τρεις ολοκληρωμένες εφαρμογές που
συγκεντρώνουν όλα όσα μάθαμε, για τη διαχείριση και την οπτικοποίηση δεδομένων.
Μπορείτε να χρησιμοποιήσετε αυτές τις εφαρμογές ως δείγμα κώδικα για να
ξεκινήσετε την επίλυση ενός πραγματικού προβλήματος.

Κάθε μία από τις εφαρμογές είναι ένα αρχείο ZIP, που μπορείτε να κατεβάσετε και
να εξαγάγετε στον υπολογιστή σας καθώς και να το εκτελέσετε.

Δημιουργία ενός OpenStreetMap από γεωκωδικοποιημένα δεδομένα
------------------------------------------------------------

\index{Google!map}
\index{Google!χάρτης}
\index{OpenStreetMap}
\index{Οπτικοποίηση!χάρτης}

Σε αυτό το πρότζεκτ, χρησιμοποιούμε το API γεωκωδικοποίησης OpenStreetMap για
να καθαρίσουμε ορισμένες γεωγραφικές τοποθεσίες, που έχουν εισαχθεί από τους
χρήστες με ονόματα πανεπιστημίων και, στη συνέχεια, να τοποθετήσουμε τα
δεδομένα σε ένα πραγματικό OpenStreetMap.

![Ένα OpenStreetMap](../images/openstreet-map)

Για να ξεκινήσετε, κατεβάστε την εφαρμογή από:

[www.gr.py4e.com/code3/opengeo.zip](http://www.gr.py4e.com/code3/opengeo.zip)

Το πρώτο πρόβλημα που πρέπει να λύσουμε είναι ότι αυτά τα API γεωκωδικοποίησης
είναι περιορισμένου ρυθμού, με έναν ορισμένο αριθμό αιτημάτων ανά ημέρα. Εάν
έχετε πολλά δεδομένα, ίσως χρειαστεί να διακόψετε και να επανεκκινήσετε τη
διαδικασία αναζήτησης αρκετές φορές. Έτσι χωρίζουμε το πρόβλημα σε δύο φάσεις.

\index{cache}

Σε πρώτη φάση παίρνουμε τα δεδομένα εισόδου τη "έρευνάς" μας από το αρχείο
*where.data*, τα διαβάζουμε μία γραμμή τη φορά, ανακτούμε τις
γεωκωδικοποιημένες πληροφορίες από την Google και τις αποθηκεύουμε σε μια βάση
δεδομένων, τη *geodata.sqlite*. Προτού χρησιμοποιήσουμε το API γεωκωδικοποίησης,
για κάθε τοποθεσία που έχει εισαγάγει ο χρήστης, απλώς ελέγχουμε να δούμε αν
έχουμε ήδη αποθηκεύσει τα δεδομένα για τη συγκεκριμένη γραμμή εισαγωγής. Η βάση
δεδομένων λειτουργεί ως τοπική "κρυφή μνήμη" των δεδομένων γεωκωδικοποίησής μας,
για να διασφαλίσουμε ότι δεν θα ζητήσουμε ποτέ από την Google τα ίδια δεδομένα
δύο φορές.

Μπορείτε να επανεκκινήσετε τη διαδικασία ανά πάσα στιγμή, αφαιρώντας το αρχείο
*geodata.sqlite*.

Εκτελέστε το πρόγραμμα *geoload.py*. Αυτό το πρόγραμμα θα διαβάσει τις γραμμές
εισόδου στο *where.data* και για κάθε γραμμή θα ελέγξει αν βρίσκεται ήδη στη
βάση δεδομένων. Εάν δεν έχουμε τα δεδομένα για την συγκεκριμένη τοποθεσία, θα
καλέσει το API γεωκωδικοποίησης για να ανακτήσει τα δεδομένα και να τα
αποθηκεύσει στη βάση δεδομένων.

Ακολουθεί ένα δείγμα εκτέλεσης, ενώ έχουν ήδη εισαχθει κάποια δεδομένα στη
βάση δεδομένων:

~~~~{text}
Βρέθηκε στη βάση δεδομένων AGH University of Science and Technology

Βρέθηκε στη βάση δεδομένων Academy of Fine Arts Warsaw Poland

Βρέθηκε στη βάση δεδομένων American University in Cairo

Βρέθηκε στη βάση δεδομένων Arizona State University

Βρέθηκε στη βάση δεδομένων Athens Information Technology

Ανάκτηση της https://py4e-data.dr-chuck.net/
   opengeo?q=BITS+Pilani
Ανακτήθηκαν 794 χαρακτήρες {"type":"FeatureColl

Ανάκτηση της https://py4e-data.dr-chuck.net/
   opengeo?q=Babcock+University
Ανακτήθηκαν 760 χαρακτήρες {"type":"FeatureColl

Ανάκτηση της https://py4e-data.dr-chuck.net/
   opengeo?q=Banaras+Hindu+University
Ανακτήθηκαν 866 χαρακτήρες {"type":"FeatureColl
...
~~~~

Οι πρώτες πέντε τοποθεσίες βρίσκονται ήδη στη βάση δεδομένων και έτσι
παραλείπονται. Το πρόγραμμα σαρώνει μέχρι το σημείο όπου βρίσκει νέες
τοποθεσίες και ξεκινά την ανάκτησή τους.

Το πρόγραμμα *geoload.py* μπορεί να διακοπεί ανά πάσα στιγμή και υπάρχει ένας
μετρητής που μπορείτε να χρησιμοποιήσετε για να περιορίσετε τον αριθμό των
κλήσεων στο API γεωκωδικοποίησης για κάθε εκτέλεση. Δεδομένου ότι το
*where.data* έχει μόνο μερικές εκατοντάδες στοιχεία δεδομένων, δεν θα πρέπει να
φτάσετε το ημερήσιο όριο ρυθμού, αλλά εάν είχατε περισσότερα δεδομένα,
ενδέχεται να χρειαστούν αρκετές εκτελέσεις, σε αρκετές ημέρες, για να φτάσει η
βάση δεδομένων σας να έχει όλα τα γεωκωδικοποιημένα δεδομένα για τα δεδομένα
εισαγωγής σας.

Αφού φορτώσετε ορισμένα δεδομένα στο *geodata.sqlite*, μπορείτε να
οπτικοποιήσετε τα δεδομένα χρησιμοποιώντας το πρόγραμμα *geodump.py*. Αυτό το
πρόγραμμα διαβάζει τη βάση δεδομένων και δημιουργεί το αρχείο *where.js* με τη
θέση, το γεωγραφικό πλάτος και μήκος, με τη μορφή εκτελέσιμου κώδικα JavaScript.

Μια εκτέλεση του προγράμματος *geodump.py* είναι η εξής:

~~~~{text}
AGH University of Science and Technology, Czarnowiejska,
Czarna Wieś, Krowodrza, Kraków, Lesser Poland
Voivodeship, 31-126, Poland 50.0657 19.91895

Academy of Fine Arts, Krakowskie Przedmieście,
Northern Śródmieście, Śródmieście, Warsaw, Masovian
Voivodeship, 00-046, Poland 52.239 21.0155
...
Υπήρχαν 260 εγραφές στο where.js
Ανοίξτε το where.html για να προβάλετε τα δεδομένα σε ένα πρόγραμμα περιήγησης
~~~~

Το αρχείο *where.html* αποτελείται από HTML και JavaScript, για την
οπτικοποίηση ενός χάρτη Google. Διαβάζει τα πιο πρόσφατα δεδομένα στο
*where.js* για να οπτικοποιήσει τα δεδομένα. Ακολουθεί η μορφή του αρχείου
*where.js*:

~~~~ {.js}
myData = [
[50.0657,19.91895,
'AGH University of Science and Technology, Czarnowiejska,
Czarna Wieś, Krowodrza, Kraków, Lesser Poland
Voivodeship, 31-126, Poland '],
[52.239,21.0155,
'Academy of Fine Arts, Krakowskie Przedmieściee,
Śródmieście Północne, Śródmieście, Warsaw,
Masovian Voivodeship, 00-046, Poland'],
   ...
];
~~~~

Αυτή είναι μια μεταβλητή JavaScript που περιέχει μια λίστα λιστών. Η σύνταξη
για τις σταθερές λίστας JavaScript είναι πολύ παρόμοια με την Python, επομένως
η σύνταξη θα πρέπει να σας είναι οικεία.

Απλώς ανοίξτε το *where.html*, σε ένα πρόγραμμα περιήγησης, για να δείτε τις
τοποθεσίες. Μπορείτε να τοποθετήσετε τον δείκτη του ποντικιού πάνω από κάθε
καρφίτσα του χάρτη, για να βρείτε την τοποθεσία που επέστρεψε το API
γεωκωδικοποίησης για την είσοδο που εισήγαγε ο χρήστης. Εάν δεν μπορείτε να
δείτε τα δεδομένα όταν ανοίγετε το αρχείο *where.html*, ίσως χρειαστεί να
ελέγξετε το JavaScript ή την κονσόλα προγραμματιστή, για το πρόγραμμα
περιήγησής σας.

Οπτικοποίηση δικτύων και διασυνδέσεων
-----------------------------------------

\index{Google!κατάταξη σελίδας}
\index{Οπτικοποίηση!δίκτυα}
\index{Οπτικοποίηση!κατάταξη σελίδας}

Σε αυτήν την εφαρμογή, θα εκτελέσουμε ορισμένες από τις λειτουργίες μιας
μηχανής αναζήτησης. Αρχικά θα δημιουργήσουμε ένα μικρό υποσύνολο του ιστού και
θα εκτελέσουμε μια απλοποιημένη έκδοση του αλγόριθμου κατάταξης σελίδων της
Google, για να προσδιορίσουμε ποιες σελίδες είναι πιο συνδεδεμένες και, στη
συνέχεια, θα οπτικοποιήσουμε την κατάταξη σελίδας και τη συνδεσιμότητα της
μικρής μας γωνιάς του ιστού. Θα χρησιμοποιήσουμε τη βιβλιοθήκη οπτικοποίησης
JavaScript D3 <http://d3js.org/>, για να παράγουμε την έξοδο οπτικοποίησης.

Μπορείτε να κατεβάσετε και να εξαγάγετε αυτήν την εφαρμογή από:

[www.gr.py4e.com/code3/pagerank.zip](http://www.gr.py4e.com/code3/pagerank.zip)

![Κατάταξη σελίδας](height=3.5in@../images/pagerank)

Το πρώτο πρόγραμμα προγράμματος (*spider.py*) ανιχνεύει έναν ιστότοπο και
καταχωρεί μια σειρά σελίδων στη βάση δεδομένων (*spider.sqlite*),
καταγράφοντας τους συνδέσμους μεταξύ των σελίδων. Μπορείτε να επανεκκινήσετε τη
διαδικασία ανά πάσα στιγμή, αφαιρώντας το αρχείο *spider.sqlite* και εκτελώντας
ξανά το *spider.py*.

~~~~{text}
Εισαγάγετε τη διεύθυνση url ιστού ή enter: http://www.dr-chuck.com/
['http://www.dr-chuck.com']
Πόσες σελίδες:2
1 http://www.dr-chuck.com/ 12
2 http://www.dr-chuck.com/csev-blog/ 57
Πόσες σελίδες:
~~~~

Σε αυτό το δείγμα εκτέλεσης, ζητήσαμε να ανιχνεύσει έναν ιστότοπο και να
ανακτήσει δύο σελίδες. Εάν κάνετε επανεκκίνηση του προγράμματος και του πείτε
να ανιχνεύσει περισσότερες σελίδες, δεν θα ανιχνεύσει ξανά καμία σελίδα που
υπάρχει ήδη στη βάση δεδομένων. Με την επανεκκίνηση πηγαίνει σε μια τυχαία
σελίδα που δεν ανιχνεύθηκε και ξεκινά από εκεί. Έτσι, κάθε διαδοχική εκτέλεση
του *spider.py* λειτουργεί προσθετικά.

~~~~{text}
Εισαγάγετε τη διεύθυνση url ιστού ή enter: http://www.dr-chuck.com/
['http://www.dr-chuck.com']
Πόσες σελίδες:3
3 http://www.dr-chuck.com/csev-blog 57
4 http://www.dr-chuck.com/dr-chuck/resume/speaking.htm 1
5 http://www.dr-chuck.com/dr-chuck/resume/index.htm 13
Πόσες σελίδες:
~~~~

Μπορείτε να έχετε πολλαπλά σημεία εκκίνησης στην ίδια βάση δεδομένων — εντός
του προγράμματος, αυτά ονομάζονται "ιστοί". Το spider επιλέγει τυχαία μεταξύ
όλων των συνδέσμων που δεν έχουν επισκεφτεί, σε όλους τους ιστούς, για να
καθορίσει την επόμενη σελίδα προς ανίχνευση.

Εάν θέλετε να καταργήσετε τα περιεχόμενα του αρχείου *spider.sqlite*, μπορείτε
να εκτελέσετε το *spdump.py* ως εξής:

~~~~{text}
(5, None, 1.0, 3, 'http://www.dr-chuck.com/csev-blog')
(3, None, 1.0, 4, 'http://www.dr-chuck.com/dr-chuck/resume/speaking.htm')
(1, None, 1.0, 2, 'http://www.dr-chuck.com/csev-blog/')
(1, None, 1.0, 5, 'http://www.dr-chuck.com/dr-chuck/resume/index.htm')
4 γραμμές.
~~~~

This shows the number of incoming links, the old page rank, the new page
rank, the id of the page, and the url of the page. The
*spdump.py* program only shows pages that have at least
one incoming link to them.

Once you have a few pages in the database, you can run page rank on the
pages using the *sprank.py* program. You simply tell it
how many page rank iterations to run.

~~~~{text}
How many iterations:2
1 0.546848992536
2 0.226714939664
[(1, 0.559), (2, 0.659), (3, 0.985), (4, 2.135), (5, 0.659)]
~~~~

You can dump the database again to see that page rank has been updated:

~~~~{text}
(5, 1.0, 0.985, 3, 'http://www.dr-chuck.com/csev-blog')
(3, 1.0, 2.135, 4, 'http://www.dr-chuck.com/dr-chuck/resume/speaking.htm')
(1, 1.0, 0.659, 2, 'http://www.dr-chuck.com/csev-blog/')
(1, 1.0, 0.659, 5, 'http://www.dr-chuck.com/dr-chuck/resume/index.htm')
4 rows.
~~~~

You can run *sprank.py* as many times as you like and it
will simply refine the page rank each time you run it. You can even run
*sprank.py* a few times and then go spider a few more
pages with *spider.py* and then run
*sprank.py* to reconverge the page rank values. A search
engine usually runs both the crawling and ranking programs all the time.

If you want to restart the page rank calculations without respidering
the web pages, you can use *spreset.py* and then restart
*sprank.py*.

~~~~{text}
How many iterations:50
1 0.546848992536
2 0.226714939664
3 0.0659516187242
4 0.0244199333
5 0.0102096489546
6 0.00610244329379
...
42 0.000109076928206
43 9.91987599002e-05
44 9.02151706798e-05
45 8.20451504471e-05
46 7.46150183837e-05
47 6.7857770908e-05
48 6.17124694224e-05
49 5.61236959327e-05
50 5.10410499467e-05
[(512, 0.0296), (1, 12.79), (2, 28.93), (3, 6.808), (4, 13.46)]
~~~~

For each iteration of the page rank algorithm it prints the average
change in page rank per page. The network initially is quite unbalanced
and so the individual page rank values change wildly between iterations.
But in a few short iterations, the page rank converges. You should run
*sprank.py* long enough that the page rank values
converge.

If you want to visualize the current top pages in terms of page rank,
run *spjson.py* to read the database and write the data
for the most highly linked pages in JSON format to be viewed in a web
browser.

~~~~{text}
Creating JSON output on spider.json...
How many nodes? 30
Open force.html in a browser to view the visualization
~~~~

You can view this data by opening the file *force.html*
in your web browser. This shows an automatic layout of the nodes and
links. You can click and drag any node and you can also double-click on
a node to find the URL that is represented by the node.

If you rerun the other utilities, rerun *spjson.py* and
press refresh in the browser to get the new data from
*spider.json*.

Visualizing mail data
---------------------

Up to this point in the book, you have become quite familiar with our
*mbox-short.txt* and *mbox.txt* data
files. Now it is time to take our analysis of email data to the next
level.

In the real world, sometimes you have to pull down mail data from
servers. That might take quite some time and the data might be
inconsistent, error-filled, and need a lot of cleanup or adjustment. In
this section, we work with an application that is the most complex so
far and pull down nearly a gigabyte of data and visualize it.

![A Word Cloud from the Sakai Developer List](height=3.5in@../images/wordcloud)

You can download this application from:

[https://www.py4e.com/code3/gmane.zip](https://www.py4e.com/code3/gmane.zip)

We will be using data from a free email list archiving service called
[http://www.gmane.org](http://www.gmane.org). This service is very popular with open
source projects because it provides a nice searchable archive of their
email activity. They also have a very liberal policy regarding accessing
their data through their API. They have no rate limits, but ask that you
don't overload their service and take only the data you need. You can
read gmane's terms and conditions at this page:

[http://www.gmane.org/export.php](http://www.gmane.org/export.php)

*It is very important that you make use of the gmane.org data
responsibly by adding delays to your access of their services and
spreading long-running jobs over a longer period of time. Do not abuse
this free service and ruin it for the rest of us.*

When the Sakai email data was spidered using this software, it produced
nearly a Gigabyte of data and took a number of runs on several days. The
file *README.txt* in the above ZIP may have instructions
as to how you can download a pre-spidered copy of the
*content.sqlite* file for a majority of the Sakai email
corpus so you don't have to spider for five days just to run the
programs. If you download the pre-spidered content, you should still run
the spidering process to catch up with more recent messages.

The first step is to spider the gmane repository. The base URL is
hard-coded in the *gmane.py* and is hard-coded to the
Sakai developer list. You can spider another repository by changing that
base url. Make sure to delete the *content.sqlite* file
if you switch the base url.

The *gmane.py* file operates as a responsible caching
spider in that it runs slowly and retrieves one mail message per second
so as to avoid getting throttled by gmane. It stores all of its data in
a database and can be interrupted and restarted as often as needed. It
may take many hours to pull all the data down. So you may need to
restart several times.

Here is a run of *gmane.py* retrieving the last five
messages of the Sakai developer list:

~~~~{text}
How many messages:10
http://download.gmane.org/gmane.comp.cms.sakai.devel/51410/51411 9460
    nealcaidin@sakaifoundation.org 2013-04-05 re: [building ...
http://download.gmane.org/gmane.comp.cms.sakai.devel/51411/51412 3379
    samuelgutierrezjimenez@gmail.com 2013-04-06 re: [building ...
http://download.gmane.org/gmane.comp.cms.sakai.devel/51412/51413 9903
    da1@vt.edu 2013-04-05 [building sakai] melete 2.9 oracle ...
http://download.gmane.org/gmane.comp.cms.sakai.devel/51413/51414 349265
    m.shedid@elraed-it.com 2013-04-07 [building sakai] ...
http://download.gmane.org/gmane.comp.cms.sakai.devel/51414/51415 3481
    samuelgutierrezjimenez@gmail.com 2013-04-07 re: ...
http://download.gmane.org/gmane.comp.cms.sakai.devel/51415/51416 0

Does not start with From
~~~~

The program scans *content.sqlite* from one up to the
first message number not already spidered and starts spidering at that
message. It continues spidering until it has spidered the desired number
of messages or it reaches a page that does not appear to be a properly
formatted message.

Sometimes __gmane.org__ is missing a message. Perhaps
administrators can delete messages or perhaps they get lost. If your
spider stops, and it seems it has hit a missing message, go into the
SQLite Manager and add a row with the missing id leaving all the other
fields blank and restart *gmane.py*. This will unstick
the spidering process and allow it to continue. These empty messages
will be ignored in the next phase of the process.

One nice thing is that once you have spidered all of the messages and
have them in *content.sqlite*, you can run
*gmane.py* again to get new messages as they are sent to
the list.

The *content.sqlite* data is pretty raw, with an
inefficient data model, and not compressed. This is intentional as it
allows you to look at *content.sqlite* in the SQLite
Manager to debug problems with the spidering process. It would be a bad
idea to run any queries against this database, as they would be quite
slow.

The second process is to run the program *gmodel.py*.
This program reads the raw data from *content.sqlite* and
produces a cleaned-up and well-modeled version of the data in the file
*index.sqlite*. This file will be much smaller (often 10X
smaller) than *content.sqlite* because it also compresses
the header and body text.

Each time *gmodel.py* runs it deletes and rebuilds
*index.sqlite*, allowing you to adjust its parameters and
edit the mapping tables in *content.sqlite* to tweak the
data cleaning process. This is a sample run of
*gmodel.py*. It prints a line out each time 250 mail
messages are processed so you can see some progress happening, as this
program may run for a while processing nearly a Gigabyte of mail data.

~~~~{text}
Loaded allsenders 1588 and mapping 28 dns mapping 1
1 2005-12-08T23:34:30-06:00 ggolden22@mac.com
251 2005-12-22T10:03:20-08:00 tpamsler@ucdavis.edu
501 2006-01-12T11:17:34-05:00 lance@indiana.edu
751 2006-01-24T11:13:28-08:00 vrajgopalan@ucmerced.edu
...
~~~~

The *gmodel.py* program handles a number of data cleaning
tasks.

Domain names are truncated to two levels for .com, .org, .edu, and .net.
Other domain names are truncated to three levels. So si.umich.edu
becomes umich.edu and caret.cam.ac.uk becomes cam.ac.uk. Email addresses
are also forced to lower case, and some of the @gmane.org address like
the following

~~~~{text}
arwhyte-63aXycvo3TyHXe+LvDLADg@public.gmane.org
~~~~

are converted to the real address whenever there is a matching real
email address elsewhere in the message corpus.

In the *mapping.sqlite* database there are two tables
that allow you to map both domain names and individual email addresses
that change over the lifetime of the email list. For example, Steve
Githens used the following email addresses as he changed jobs over the
life of the Sakai developer list:

~~~~{text}
s-githens@northwestern.edu
sgithens@cam.ac.uk
swgithen@mtu.edu
~~~~

We can add two entries to the Mapping table in
*mapping.sqlite* so *gmodel.py* will map
all three to one address:

~~~~{text}
s-githens@northwestern.edu ->  swgithen@mtu.edu
sgithens@cam.ac.uk -> swgithen@mtu.edu
~~~~

You can also make similar entries in the DNSMapping table if there are
multiple DNS names you want mapped to a single DNS. The following
mapping was added to the Sakai data:

~~~~{text}
iupui.edu -> indiana.edu
~~~~

so all the accounts from the various Indiana University campuses are
tracked together.

You can rerun the *gmodel.py* over and over as you look
at the data, and add mappings to make the data cleaner and cleaner. When
you are done, you will have a nicely indexed version of the email in
*index.sqlite*. This is the file to use to do data
analysis. With this file, data analysis will be really quick.

The first, simplest data analysis is to determine "who sent the most
mail?" and "which organization sent the most mail"? This is done using
*gbasic.py*:

~~~~{text}
How many to dump? 5
Loaded messages= 51330 subjects= 25033 senders= 1584

Top 5 Email list participants
steve.swinsburg@gmail.com 2657
azeckoski@unicon.net 1742
ieb@tfd.co.uk 1591
csev@umich.edu 1304
david.horwitz@uct.ac.za 1184

Top 5 Email list organizations
gmail.com 7339
umich.edu 6243
uct.ac.za 2451
indiana.edu 2258
unicon.net 2055
~~~~

Note how much more quickly *gbasic.py* runs compared to
*gmane.py* or even *gmodel.py*. They are
all working on the same data, but *gbasic.py* is using
the compressed and normalized data in *index.sqlite*. If
you have a lot of data to manage, a multistep process like the one in
this application may take a little longer to develop, but will save you
a lot of time when you really start to explore and visualize your data.

You can produce a simple visualization of the word frequency in the
subject lines in the file *gword.py*:

~~~~{text}
Range of counts: 33229 129
Output written to gword.js
~~~~

This produces the file *gword.js* which you can visualize
using *gword.htm* to produce a word cloud similar to the
one at the beginning of this section.

A second visualization is produced by *gline.py*. It
computes email participation by organizations over time.

~~~~{text}
Loaded messages= 51330 subjects= 25033 senders= 1584
Top 10 Oranizations
['gmail.com', 'umich.edu', 'uct.ac.za', 'indiana.edu',
'unicon.net', 'tfd.co.uk', 'berkeley.edu', 'longsight.com',
'stanford.edu', 'ox.ac.uk']
Output written to gline.js
~~~~

Its output is written to *gline.js* which is visualized
using *gline.htm*.

![Sakai Mail Activity by Organization](../images/mailorg)

This is a relatively complex and sophisticated application and has
features to do some real data retrieval, cleaning, and visualization.
